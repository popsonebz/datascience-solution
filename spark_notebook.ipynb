{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the csv path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "csv_path = os.environ[\"CSV_FILE_DIR\"]\n",
    "if csv_path.endswith(\"/\"):\n",
    "     csv_path = csv_path + \"data.csv\"   \n",
    "else:\n",
    "    csv_path = csv_path + \"/data.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"running bash command to check if file is there\")\n",
    "! ls -la $csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T09:28:44.176687Z",
     "start_time": "2018-01-05T09:28:44.159927Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Spark Library\n",
    "from pyspark.sql import SparkSession, functions as sqlfun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note: Depending on how you have configured Spark in your spark config files and depending on how you defined spark w.r.t jupyter, the following line of code might not work and/or be needed explicitly for the other code to run. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create Spark Session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Read CSV\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T09:29:34.567132Z",
     "start_time": "2018-01-05T09:29:34.409334Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read the csv file\n",
    "try:\n",
    "    csv_dataframe = spark.read.csv(\"file:///csv_path\",header=True)\n",
    "    print(\"CSV read into Spark Data Frame\")\n",
    "except:\n",
    "    print(\"ERROR: Unable to load file\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T09:28:50.633154Z",
     "start_time": "2018-01-05T09:28:50.548360Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T09:28:51.445305Z",
     "start_time": "2018-01-05T09:28:51.432419Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T09:34:33.861821Z",
     "start_time": "2018-01-05T09:34:33.799923Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a new dataframe with a better schema \n",
    "#root\n",
    "# |-- age: integer (nullable = true)\n",
    "# |-- birthday: date (nullable = true)\n",
    "# |-- zip: string (nullable = true)\n",
    "# |-- city: string (nullable = true)\n",
    "\n",
    "csv_dataframe_new_schema = csv_dataframe.select(sqlfun.col(\"age\").cast(\"integer\"),\n",
    "                                                sqlfun.from_unixtime(sqlfun.unix_timestamp('birthday', 'MM/dd/yyy')).cast('date').alias(\"birthday\"),\n",
    "                                                \"zip\",\n",
    "                                                \"city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T09:34:42.436907Z",
     "start_time": "2018-01-05T09:34:42.423887Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_dataframe_new_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T09:34:47.945282Z",
     "start_time": "2018-01-05T09:34:47.853941Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_dataframe_new_schema.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35_dev",
   "language": "python",
   "name": "py35_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
